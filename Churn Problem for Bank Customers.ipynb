{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREVENTING BANK CUSTOMER CHURN!\n",
    "\" Exploratory Data Analysis & Prediction!\"\n",
    "2018\n",
    "[ by_Keldine Malit ]\n",
    "\n",
    "1. Introduction\n",
    "We aim to accomplist the following for this study:\n",
    "\n",
    "Identify and visualize which factors contribute to customer churn:\n",
    "\n",
    "Build a prediction model that will perform the following:\n",
    "\n",
    "Classify if a customer is going to churn or not\n",
    "Preferably and based on model performance, choose a model that will attach a probability to the churn to make it easier for customer service to target low hanging fruits in their efforts to prevent churn\n",
    "2. Data set review & preparation\n",
    "In this section we will seek to explore the structure of our data:\n",
    "\n",
    "To understand the input space the data set\n",
    "And to prepare the sets for exploratory and prediction tasks as described in section 1\n",
    "## REQUIRED LIBRARIES\n",
    "# For data wrangling \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "# Read the data frame\n",
    "df = pd.read_csv('../input/Churn_Modelling.csv', delimiter=',')\n",
    "df.shape\n",
    "(10000, 14)\n",
    "The Df has 1000 rows with 14 attributes. We review this further to identify what attributes will be necessary and what data manipulation needs to be carried out before Exploratory analysis and prediction modelling\n",
    "\n",
    "# Check columns list and missing values\n",
    "df.isnull().sum()\n",
    "RowNumber          0\n",
    "CustomerId         0\n",
    "Surname            0\n",
    "CreditScore        0\n",
    "Geography          0\n",
    "Gender             0\n",
    "Age                0\n",
    "Tenure             0\n",
    "Balance            0\n",
    "NumOfProducts      0\n",
    "HasCrCard          0\n",
    "IsActiveMember     0\n",
    "EstimatedSalary    0\n",
    "Exited             0\n",
    "dtype: int64\n",
    "Well isn't that a rare find; no missing values!\n",
    "\n",
    "# Get unique count for each variable\n",
    "df.nunique()\n",
    "RowNumber          10000\n",
    "CustomerId         10000\n",
    "Surname             2932\n",
    "CreditScore          460\n",
    "Geography              3\n",
    "Gender                 2\n",
    "Age                   70\n",
    "Tenure                11\n",
    "Balance             6382\n",
    "NumOfProducts          4\n",
    "HasCrCard              2\n",
    "IsActiveMember         2\n",
    "EstimatedSalary     9999\n",
    "Exited                 2\n",
    "dtype: int64\n",
    "From the above, we will not require the first 2 attributes as the are specific to a customer. It is borderline with the surname as this would result to profiling so we exclude this as well.\n",
    "\n",
    "# Drop the columns as explained above\n",
    "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)\n",
    "# Review the top rows of what is left of the data frame\n",
    "df.head()\n",
    "CreditScore\tGeography\tGender\tAge\tTenure\tBalance\tNumOfProducts\tHasCrCard\tIsActiveMember\tEstimatedSalary\tExited\n",
    "0\t619\tFrance\tFemale\t42\t2\t0.00\t1\t1\t1\t101348.88\t1\n",
    "1\t608\tSpain\tFemale\t41\t1\t83807.86\t1\t0\t1\t112542.58\t0\n",
    "2\t502\tFrance\tFemale\t42\t8\t159660.80\t3\t1\t0\t113931.57\t1\n",
    "3\t699\tFrance\tFemale\t39\t1\t0.00\t2\t0\t0\t93826.63\t0\n",
    "4\t850\tSpain\tFemale\t43\t2\t125510.82\t1\t1\t1\t79084.10\t0\n",
    "From the above, a couple of question linger:\n",
    "\n",
    "The data appears to be a snapshot as some point in time e.g. the balance is for a given date which leaves a lot of questions:\n",
    "What date is it and of what relevance is this date\n",
    "Would it be possible to obtain balances over a period of time as opposed to a single date.\n",
    "There are customers who have exited but still have a balance in their account! What would this mean? Could they have exited from a product and not the bank?\n",
    "What does being an active member mean and are there difference degrees to it? Could it be better to provide transaction count both in terms of credits and debits to the account instead?\n",
    "A break down to the products bought into by a customer could provide more information topping listing of product count\n",
    "For this exercise, we proceed to model without context even though typically having context and better understanding of the data extraction process would give better insight and possibly lead to better and contextual results of the modelling process\n",
    "\n",
    "# Check variable data types\n",
    "df.dtypes\n",
    "CreditScore          int64\n",
    "Geography           object\n",
    "Gender              object\n",
    "Age                  int64\n",
    "Tenure               int64\n",
    "Balance            float64\n",
    "NumOfProducts        int64\n",
    "HasCrCard            int64\n",
    "IsActiveMember       int64\n",
    "EstimatedSalary    float64\n",
    "Exited               int64\n",
    "dtype: object\n",
    "So we moslty have categorical variables and 5 continuous variables\n",
    "\n",
    "3. Exploratory Data Analysis\n",
    "Here our main interest is to get an understanding as to how the given attributes relate too the 'Exit' status.\n",
    "\n",
    "labels = 'Exited', 'Retained'\n",
    "sizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\n",
    "explode = (0, 0.1)\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Proportion of customer churned and retained\", size = 20)\n",
    "plt.show()\n",
    "\n",
    "So about 20% of the customers have churned. So the baseline model could be to predict that 20% of the customers will churn. Given 20% is a small number, we need to ensure that the chosen model does predict with great accuracy this 20% as it is of interest to the bank to identify and keep this bunch as opposed to accurately predicting the customers that are retained.\n",
    "\n",
    "    # We first review the 'Status' relation with categorical variables\n",
    "    fig, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    sns.countplot(x='Geography', hue = 'Exited',data = df, ax=axarr[0][0])\n",
    "    sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[0][1])\n",
    "    sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[1][0])\n",
    "    sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[1][1])\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a5d3f2780>\n",
    "\n",
    "We note the following:\n",
    "\n",
    "Majority of the data is from persons from France. However, the proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem (maybe not enough customer service resources allocated) in the areas where it has fewer clients.\n",
    "The proportion of female customers churning is also greater than that of male customers\n",
    "Interestingly, majority of the customers that churned are those with credit cards. Given that majority of the customers have credit cards could prove this to be just a coincidence.\n",
    "Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn.\n",
    "    # Relations based on the continuous data attributes\n",
    "    fig, axarr = plt.subplots(3, 2, figsize=(20, 12))\n",
    "    sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0])\n",
    "    sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1])\n",
    "    sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\n",
    "    sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\n",
    "    sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\n",
    "    sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a5ca8bc18>\n",
    "\n",
    "We note the following:\n",
    "\n",
    "There is no significant difference in the credit score distribution between retained and churned customers.\n",
    "The older customers are churning at more than the younger ones alluding to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups\n",
    "With regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.\n",
    "Worryingly, the bank is losing customers with significant bank balances which is likely to hit their available capital for lending.\n",
    "Neither the product nor the salary has a significant effect on the likelihood to churn.\n",
    "4. Feature engineering\n",
    "We seek to add features that are likely to have an impact on the probability of churning. We first split the train and test sets\n",
    "\n",
    "# Split Train, test data\n",
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "8000\n",
    "2000\n",
    "df_train['BalanceSalaryRatio'] = df_train.Balance/df_train.EstimatedSalary\n",
    "sns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df_train)\n",
    "plt.ylim(-1, 5)\n",
    "(-1, 5)\n",
    "\n",
    "we have seen that the salary has little effect on the chance of a customer churning. However as seen above, the ratio of the bank balance and the estimated salary indicates that customers with a higher balance salary ratio churn more which would be worrying to the bank as this impacts their source of loan capital.\n",
    "\n",
    "# Given that tenure is a 'function' of age, we introduce a variable aiming to standardize tenure over age:\n",
    "df_train['TenureByAge'] = df_train.Tenure/(df_train.Age)\n",
    "sns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df_train)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()\n",
    "\n",
    "'''Lastly we introduce a variable to capture credit score given age to take into account credit behaviour visavis adult life\n",
    ":-)'''\n",
    "df_train['CreditScoreGivenAge'] = df_train.CreditScore/(df_train.Age)\n",
    "# Resulting Data Frame\n",
    "df_train.head()\n",
    "CreditScore\tGeography\tGender\tAge\tTenure\tBalance\tNumOfProducts\tHasCrCard\tIsActiveMember\tEstimatedSalary\tExited\tBalanceSalaryRatio\tTenureByAge\tCreditScoreGivenAge\n",
    "8159\t461\tSpain\tFemale\t25\t6\t0.00\t2\t1\t1\t15306.29\t0\t0.000000\t0.240000\t18.440000\n",
    "6332\t619\tFrance\tFemale\t35\t4\t90413.12\t1\t1\t1\t20555.21\t0\t4.398550\t0.114286\t17.685714\n",
    "8895\t699\tFrance\tFemale\t40\t8\t122038.34\t1\t1\t0\t102085.35\t0\t1.195454\t0.200000\t17.475000\n",
    "5351\t558\tGermany\tMale\t41\t2\t124227.14\t1\t1\t1\t111184.67\t0\t1.117305\t0.048780\t13.609756\n",
    "4314\t638\tFrance\tMale\t34\t5\t133501.36\t1\t0\t1\t155643.04\t0\t0.857741\t0.147059\t18.764706\n",
    "5. Data prep for model fitting\n",
    "# Arrange columns by data type for easier manipulation\n",
    "continuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n",
    "                   'TenureByAge','CreditScoreGivenAge']\n",
    "cat_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']\n",
    "df_train = df_train[['Exited'] + continuous_vars + cat_vars]\n",
    "df_train.head()\n",
    "Exited\tCreditScore\tAge\tTenure\tBalance\tNumOfProducts\tEstimatedSalary\tBalanceSalaryRatio\tTenureByAge\tCreditScoreGivenAge\tHasCrCard\tIsActiveMember\tGeography\tGender\n",
    "8159\t0\t461\t25\t6\t0.00\t2\t15306.29\t0.000000\t0.240000\t18.440000\t1\t1\tSpain\tFemale\n",
    "6332\t0\t619\t35\t4\t90413.12\t1\t20555.21\t4.398550\t0.114286\t17.685714\t1\t1\tFrance\tFemale\n",
    "8895\t0\t699\t40\t8\t122038.34\t1\t102085.35\t1.195454\t0.200000\t17.475000\t1\t0\tFrance\tFemale\n",
    "5351\t0\t558\t41\t2\t124227.14\t1\t111184.67\t1.117305\t0.048780\t13.609756\t1\t1\tGermany\tMale\n",
    "4314\t0\t638\t34\t5\t133501.36\t1\t155643.04\t0.857741\t0.147059\t18.764706\t0\t1\tFrance\tMale\n",
    "'''For the one hot variables, we change 0 to -1 so that the models can capture a negative relation \n",
    "where the attribute in inapplicable instead of 0'''\n",
    "df_train.loc[df_train.HasCrCard == 0, 'HasCrCard'] = -1\n",
    "df_train.loc[df_train.IsActiveMember == 0, 'IsActiveMember'] = -1\n",
    "df_train.head()\n",
    "Exited\tCreditScore\tAge\tTenure\tBalance\tNumOfProducts\tEstimatedSalary\tBalanceSalaryRatio\tTenureByAge\tCreditScoreGivenAge\tHasCrCard\tIsActiveMember\tGeography\tGender\n",
    "8159\t0\t461\t25\t6\t0.00\t2\t15306.29\t0.000000\t0.240000\t18.440000\t1\t1\tSpain\tFemale\n",
    "6332\t0\t619\t35\t4\t90413.12\t1\t20555.21\t4.398550\t0.114286\t17.685714\t1\t1\tFrance\tFemale\n",
    "8895\t0\t699\t40\t8\t122038.34\t1\t102085.35\t1.195454\t0.200000\t17.475000\t1\t-1\tFrance\tFemale\n",
    "5351\t0\t558\t41\t2\t124227.14\t1\t111184.67\t1.117305\t0.048780\t13.609756\t1\t1\tGermany\tMale\n",
    "4314\t0\t638\t34\t5\t133501.36\t1\t155643.04\t0.857741\t0.147059\t18.764706\t-1\t1\tFrance\tMale\n",
    "# One hot encode the categorical variables\n",
    "lst = ['Geography', 'Gender']\n",
    "remove = list()\n",
    "for i in lst:\n",
    "    if (df_train[i].dtype == np.str or df_train[i].dtype == np.object):\n",
    "        for j in df_train[i].unique():\n",
    "            df_train[i+'_'+j] = np.where(df_train[i] == j,1,-1)\n",
    "        remove.append(i)\n",
    "df_train = df_train.drop(remove, axis=1)\n",
    "df_train.head()\n",
    "Exited\tCreditScore\tAge\tTenure\tBalance\tNumOfProducts\tEstimatedSalary\tBalanceSalaryRatio\tTenureByAge\tCreditScoreGivenAge\tHasCrCard\tIsActiveMember\tGeography_Spain\tGeography_France\tGeography_Germany\tGender_Female\tGender_Male\n",
    "8159\t0\t461\t25\t6\t0.00\t2\t15306.29\t0.000000\t0.240000\t18.440000\t1\t1\t1\t-1\t-1\t1\t-1\n",
    "6332\t0\t619\t35\t4\t90413.12\t1\t20555.21\t4.398550\t0.114286\t17.685714\t1\t1\t-1\t1\t-1\t1\t-1\n",
    "8895\t0\t699\t40\t8\t122038.34\t1\t102085.35\t1.195454\t0.200000\t17.475000\t1\t-1\t-1\t1\t-1\t1\t-1\n",
    "5351\t0\t558\t41\t2\t124227.14\t1\t111184.67\t1.117305\t0.048780\t13.609756\t1\t1\t-1\t-1\t1\t-1\t1\n",
    "4314\t0\t638\t34\t5\t133501.36\t1\t155643.04\t0.857741\t0.147059\t18.764706\t-1\t1\t-1\t1\t-1\t-1\t1\n",
    "# minMax scaling the continuous variables\n",
    "minVec = df_train[continuous_vars].min().copy()\n",
    "maxVec = df_train[continuous_vars].max().copy()\n",
    "df_train[continuous_vars] = (df_train[continuous_vars]-minVec)/(maxVec-minVec)\n",
    "df_train.head()\n",
    "Exited\tCreditScore\tAge\tTenure\tBalance\tNumOfProducts\tEstimatedSalary\tBalanceSalaryRatio\tTenureByAge\tCreditScoreGivenAge\tHasCrCard\tIsActiveMember\tGeography_Spain\tGeography_France\tGeography_Germany\tGender_Female\tGender_Male\n",
    "8159\t0\t0.222\t0.094595\t0.6\t0.000000\t0.333333\t0.076118\t0.000000\t0.432000\t0.323157\t1\t1\t1\t-1\t-1\t1\t-1\n",
    "6332\t0\t0.538\t0.229730\t0.4\t0.360358\t0.000000\t0.102376\t0.003317\t0.205714\t0.305211\t1\t1\t-1\t1\t-1\t1\t-1\n",
    "8895\t0\t0.698\t0.297297\t0.8\t0.486406\t0.000000\t0.510225\t0.000901\t0.360000\t0.300198\t1\t-1\t-1\t1\t-1\t1\t-1\n",
    "5351\t0\t0.416\t0.310811\t0.2\t0.495130\t0.000000\t0.555744\t0.000843\t0.087805\t0.208238\t1\t1\t-1\t-1\t1\t-1\t1\n",
    "4314\t0\t0.576\t0.216216\t0.5\t0.532094\t0.000000\t0.778145\t0.000647\t0.264706\t0.330882\t-1\t1\t-1\t1\t-1\t-1\t1\n",
    "# data prep pipeline for test data\n",
    "def DfPrepPipeline(df_predict,df_train_Cols,minVec,maxVec):\n",
    "    # Add new features\n",
    "    df_predict['BalanceSalaryRatio'] = df_predict.Balance/df_predict.EstimatedSalary\n",
    "    df_predict['TenureByAge'] = df_predict.Tenure/(df_predict.Age - 18)\n",
    "    df_predict['CreditScoreGivenAge'] = df_predict.CreditScore/(df_predict.Age - 18)\n",
    "    # Reorder the columns\n",
    "    continuous_vars = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','BalanceSalaryRatio',\n",
    "                   'TenureByAge','CreditScoreGivenAge']\n",
    "    cat_vars = ['HasCrCard','IsActiveMember',\"Geography\", \"Gender\"] \n",
    "    df_predict = df_predict[['Exited'] + continuous_vars + cat_vars]\n",
    "    # Change the 0 in categorical variables to -1\n",
    "    df_predict.loc[df_predict.HasCrCard == 0, 'HasCrCard'] = -1\n",
    "    df_predict.loc[df_predict.IsActiveMember == 0, 'IsActiveMember'] = -1\n",
    "    # One hot encode the categorical variables\n",
    "    lst = [\"Geography\", \"Gender\"]\n",
    "    remove = list()\n",
    "    for i in lst:\n",
    "        for j in df_predict[i].unique():\n",
    "            df_predict[i+'_'+j] = np.where(df_predict[i] == j,1,-1)\n",
    "        remove.append(i)\n",
    "    df_predict = df_predict.drop(remove, axis=1)\n",
    "    # Ensure that all one hot encoded variables that appear in the train data appear in the subsequent data\n",
    "    L = list(set(df_train_Cols) - set(df_predict.columns))\n",
    "    for l in L:\n",
    "        df_predict[str(l)] = -1        \n",
    "    # MinMax scaling coontinuous variables based on min and max from the train data\n",
    "    df_predict[continuous_vars] = (df_predict[continuous_vars]-minVec)/(maxVec-minVec)\n",
    "    # Ensure that The variables are ordered in the same way as was ordered in the train set\n",
    "    df_predict = df_predict[df_train_Cols]\n",
    "    return df_predict\n",
    "6. Model fitting and selection\n",
    "For the model fitting, I will try out the following\n",
    "\n",
    "Logistic regression in the primal space and with different kernels\n",
    "SVM in the primal and with different Kernels\n",
    "Ensemble models\n",
    "# Support functions\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Fit models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scoring functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "# Function to give best model score and parameters\n",
    "def best_model(model):\n",
    "    print(model.best_score_)    \n",
    "    print(model.best_params_)\n",
    "    print(model.best_estimator_)\n",
    "def get_auc_scores(y_actual, method,method2):\n",
    "    auc_score = roc_auc_score(y_actual, method); \n",
    "    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n",
    "    return (auc_score, fpr_df, tpr_df)\n",
    "!Warning. This section takes a loooooong time to run so you have the option to skip to the next section where I fit the better models from this section.\n",
    "# Fit primal logistic regression\n",
    "param_grid = {'C': [0.1,0.5,1,10,50,100], 'max_iter': [250], 'fit_intercept':[True],'intercept_scaling':[1],\n",
    "              'penalty':['l2'], 'tol':[0.00001,0.0001,0.000001]}\n",
    "log_primal_Grid = GridSearchCV(LogisticRegression(solver='lbfgs'),param_grid, cv=10, refit=True, verbose=0)\n",
    "log_primal_Grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "best_model(log_primal_Grid)\n",
    "0.815125\n",
    "{'C': 100, 'fit_intercept': True, 'intercept_scaling': 1, 'max_iter': 250, 'penalty': 'l2', 'tol': 1e-05}\n",
    "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=250, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
    "          tol=1e-05, verbose=0, warm_start=False)\n",
    "# Fit logistic regression with degree 2 polynomial kernel\n",
    "param_grid = {'C': [0.1,10,50], 'max_iter': [300,500], 'fit_intercept':[True],'intercept_scaling':[1],'penalty':['l2'],\n",
    "              'tol':[0.0001,0.000001]}\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "df_train_pol2 = poly2.fit_transform(df_train.loc[:, df_train.columns != 'Exited'])\n",
    "log_pol2_Grid = GridSearchCV(LogisticRegression(solver = 'liblinear'),param_grid, cv=5, refit=True, verbose=0)\n",
    "log_pol2_Grid.fit(df_train_pol2,df_train.Exited)\n",
    "best_model(log_pol2_Grid)\n",
    "0.855625\n",
    "{'C': 10, 'fit_intercept': True, 'intercept_scaling': 1, 'max_iter': 300, 'penalty': 'l2', 'tol': 0.0001}\n",
    "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=300, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "# Fit SVM with RBF Kernel\n",
    "param_grid = {'C': [0.5,100,150], 'gamma': [0.1,0.01,0.001],'probability':[True],'kernel': ['rbf']}\n",
    "SVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\n",
    "SVM_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "best_model(SVM_grid)\n",
    "0.851875\n",
    "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf', 'probability': True}\n",
    "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "# Fit SVM with pol kernel\n",
    "param_grid = {'C': [0.5,1,10,50,100], 'gamma': [0.1,0.01,0.001],'probability':[True],'kernel': ['poly'],'degree':[2,3] }\n",
    "SVM_grid = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=0)\n",
    "SVM_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "best_model(SVM_grid)\n",
    "0.8545\n",
    "{'C': 100, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly', 'probability': True}\n",
    "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "# Fit random forest classifier\n",
    "param_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [2,4,6,7,8,9],'n_estimators':[50,100],'min_samples_split': [3, 5, 6, 7]}\n",
    "RanFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, refit=True, verbose=0)\n",
    "RanFor_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "best_model(RanFor_grid)\n",
    "0.863125\n",
    "{'max_depth': 8, 'max_features': 9, 'min_samples_split': 6, 'n_estimators': 50}\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=8, max_features=9, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=6,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "# Fit Extreme Gradient boosting classifier\n",
    "param_grid = {'max_depth': [5,6,7,8], 'gamma': [0.01,0.001,0.001],'min_child_weight':[1,5,10], 'learning_rate': [0.05,0.1, 0.2, 0.3], 'n_estimators':[5,10,20,100]}\n",
    "xgb_grid = GridSearchCV(XGBClassifier(), param_grid, cv=5, refit=True, verbose=0)\n",
    "xgb_grid.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "best_model(xgb_grid)\n",
    "0.86325\n",
    "{'gamma': 0.01, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 20}\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=7, min_child_weight=5, missing=None, n_estimators=20,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "Fit best Models\n",
    "# Fit primal logistic regression\n",
    "log_primal = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='warn',n_jobs=None, \n",
    "                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)\n",
    "log_primal.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=250, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
    "          tol=1e-05, verbose=0, warm_start=False)\n",
    "# Fit logistic regression with pol 2 kernel\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "df_train_pol2 = poly2.fit_transform(df_train.loc[:, df_train.columns != 'Exited'])\n",
    "log_pol2 = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='warn', n_jobs=None, \n",
    "                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)\n",
    "log_pol2.fit(df_train_pol2,df_train.Exited)\n",
    "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=300, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "# Fit SVM with RBF Kernel\n",
    "SVM_RBF = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, \n",
    "              random_state=None, shrinking=True,tol=0.001, verbose=False)\n",
    "SVM_RBF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "# Fit SVM with Pol Kernel\n",
    "SVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,\n",
    "              probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "SVM_POL.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "# Fit Random Forest classifier\n",
    "RF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=8, max_features=6, max_leaf_nodes=None,min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None,min_samples_leaf=1, min_samples_split=3,min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "                            oob_score=False, random_state=None, verbose=0,warm_start=False)\n",
    "RF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=8, max_features=6, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=3,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "# Fit Extreme Gradient Boost Classifier\n",
    "XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,\n",
    "                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n",
    "                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)\n",
    "XGB.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=7, min_child_weight=5, missing=None, n_estimators=20,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "Review best model fit accuracy : Keen interest is on the performance in predicting 1's (Customers who churn)\n",
    "print(classification_report(df_train.Exited, log_primal.predict(df_train.loc[:, df_train.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.97      0.89      6353\n",
    "           1       0.64      0.24      0.35      1647\n",
    "\n",
    "   micro avg       0.82      0.82      0.82      8000\n",
    "   macro avg       0.73      0.60      0.62      8000\n",
    "weighted avg       0.79      0.82      0.78      8000\n",
    "\n",
    "print(classification_report(df_train.Exited,  log_pol2.predict(df_train_pol2)))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.97      0.92      6353\n",
    "           1       0.77      0.46      0.57      1647\n",
    "\n",
    "   micro avg       0.86      0.86      0.86      8000\n",
    "   macro avg       0.82      0.71      0.75      8000\n",
    "weighted avg       0.85      0.86      0.85      8000\n",
    "\n",
    "print(classification_report(df_train.Exited,  SVM_RBF.predict(df_train.loc[:, df_train.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.98      0.92      6353\n",
    "           1       0.85      0.40      0.54      1647\n",
    "\n",
    "   micro avg       0.86      0.86      0.86      8000\n",
    "   macro avg       0.86      0.69      0.73      8000\n",
    "weighted avg       0.86      0.86      0.84      8000\n",
    "\n",
    "print(classification_report(df_train.Exited,  SVM_POL.predict(df_train.loc[:, df_train.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.98      0.92      6353\n",
    "           1       0.84      0.38      0.52      1647\n",
    "\n",
    "   micro avg       0.86      0.86      0.86      8000\n",
    "   macro avg       0.85      0.68      0.72      8000\n",
    "weighted avg       0.85      0.86      0.83      8000\n",
    "\n",
    "print(classification_report(df_train.Exited,  RF.predict(df_train.loc[:, df_train.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.98      0.93      6353\n",
    "           1       0.88      0.52      0.66      1647\n",
    "\n",
    "   micro avg       0.89      0.89      0.89      8000\n",
    "   macro avg       0.88      0.75      0.79      8000\n",
    "weighted avg       0.89      0.89      0.88      8000\n",
    "\n",
    "print(classification_report(df_train.Exited,  XGB.predict(df_train.loc[:, df_train.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.97      0.93      6353\n",
    "           1       0.83      0.53      0.64      1647\n",
    "\n",
    "   micro avg       0.88      0.88      0.88      8000\n",
    "   macro avg       0.86      0.75      0.79      8000\n",
    "weighted avg       0.88      0.88      0.87      8000\n",
    "\n",
    "y = df_train.Exited\n",
    "X = df_train.loc[:, df_train.columns != 'Exited']\n",
    "X_pol2 = df_train_pol2\n",
    "auc_log_primal, fpr_log_primal, tpr_log_primal = get_auc_scores(y, log_primal.predict(X),log_primal.predict_proba(X)[:,1])\n",
    "auc_log_pol2, fpr_log_pol2, tpr_log_pol2 = get_auc_scores(y, log_pol2.predict(X_pol2),log_pol2.predict_proba(X_pol2)[:,1])\n",
    "auc_SVM_RBF, fpr_SVM_RBF, tpr_SVM_RBF = get_auc_scores(y, SVM_RBF.predict(X),SVM_RBF.predict_proba(X)[:,1])\n",
    "auc_SVM_POL, fpr_SVM_POL, tpr_SVM_POL = get_auc_scores(y, SVM_POL.predict(X),SVM_POL.predict_proba(X)[:,1])\n",
    "auc_RF, fpr_RF, tpr_RF = get_auc_scores(y, RF.predict(X),RF.predict_proba(X)[:,1])\n",
    "auc_XGB, fpr_XGB, tpr_XGB = get_auc_scores(y, XGB.predict(X),XGB.predict_proba(X)[:,1])\n",
    "plt.figure(figsize = (12,6), linewidth= 1)\n",
    "plt.plot(fpr_log_primal, tpr_log_primal, label = 'log primal Score: ' + str(round(auc_log_primal, 5)))\n",
    "plt.plot(fpr_log_pol2, tpr_log_pol2, label = 'log pol2 score: ' + str(round(auc_log_pol2, 5)))\n",
    "plt.plot(fpr_SVM_RBF, tpr_SVM_RBF, label = 'SVM RBF Score: ' + str(round(auc_SVM_RBF, 5)))\n",
    "plt.plot(fpr_SVM_POL, tpr_SVM_POL, label = 'SVM POL Score: ' + str(round(auc_SVM_POL, 5)))\n",
    "plt.plot(fpr_RF, tpr_RF, label = 'RF score: ' + str(round(auc_RF, 5)))\n",
    "plt.plot(fpr_XGB, tpr_XGB, label = 'XGB score: ' + str(round(auc_XGB, 5)))\n",
    "plt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "#plt.savefig('roc_results_ratios.png')\n",
    "plt.show()\n",
    "\n",
    "From the above results, my main aim is to predict the customers that will possibly churn so they can be put in some sort of scheme to prevent churn hence the recall measures on the 1's is of more importance to me than the overall accuracy score of the model.\n",
    "Given that in the data we only had 20% of churn, a recall greater than this baseline will already be an improvement but we want to get as high as possible while trying to maintain a high precision so that the bank can train its resources effectively towards clients highlighted by the model without wasting too much resources on the false positives.\n",
    "From the review of the fitted models above, the best model that gives a decent balance of the recall and precision is the random forest where according to the fit on the training set, with a precision score on 1's of 0.88, out of all customers that the model thinks will churn, 88% do actually churn and with the recall score of 0.53 on the 1's, the model is able to highlight 53% of all those who churned.\n",
    "Test model prediction accuracy on test data\n",
    "# Make the data transformation for test data\n",
    "df_test = DfPrepPipeline(df_test,df_train.columns,minVec,maxVec)\n",
    "df_test = df_test.mask(np.isinf(df_test))\n",
    "df_test = df_test.dropna()\n",
    "df_test.shape\n",
    "(1996, 17)\n",
    "print(classification_report(df_test.Exited,  RF.predict(df_test.loc[:, df_test.columns != 'Exited'])))\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.98      0.92      1607\n",
    "           1       0.79      0.38      0.51       389\n",
    "\n",
    "   micro avg       0.86      0.86      0.86      1996\n",
    "   macro avg       0.83      0.68      0.72      1996\n",
    "weighted avg       0.85      0.86      0.84      1996\n",
    "\n",
    "auc_RF_test, fpr_RF_test, tpr_RF_test = get_auc_scores(df_test.Exited, RF.predict(df_test.loc[:, df_test.columns != 'Exited']),\n",
    "                                                       RF.predict_proba(df_test.loc[:, df_test.columns != 'Exited'])[:,1])\n",
    "plt.figure(figsize = (12,6), linewidth= 1)\n",
    "plt.plot(fpr_RF_test, tpr_RF_test, label = 'RF score: ' + str(round(auc_RF_test, 5)))\n",
    "plt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "#plt.savefig('roc_results_ratios.png')\n",
    "plt.show()\n",
    "\n",
    "7. Conclusion\n",
    "The precision of the model on previousy unseen test data is slightly higher with regard to predicting 1's i.e. those customers that churn. However, in as much as the model has a high accuracy, it still misses about half of those who end up churning. This could be imprved by providing retraining the model with more data over time while in the meantime working with the model to save the 41% that would have churned :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
